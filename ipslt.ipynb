{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy_f7IAiu30U"
      },
      "source": [
        "### Getting ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owlfQHRMu30W",
        "outputId": "8b08cf6c-ad16-49df-a0b3-1a60bfa34e75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VideoResNet(\n",
              "  (stem): BasicStem(\n",
              "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
              "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer,MBartConfig\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased')\n",
        "from torchvision.models.video import r3d_18\n",
        "import pickle\n",
        "\n",
        "# Load the pre-trained model\n",
        "i3d = r3d_18(pretrained=True).to(\"cuda\")\n",
        "i3d.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3xTj3Hnu30Y",
        "outputId": "7df7156c-432c-4d33-d9b9-5793effeeb8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmnc-bYQu30Y",
        "outputId": "9eace541-f91e-4283-cb92-ed0aac473bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 31102\n"
          ]
        }
      ],
      "source": [
        "# Get the size of the tokenizer's vocabulary\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXXx92YBu30Z",
        "outputId": "32b81cc3-a147-404b-c899-bca994282186"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x251bd6593f0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "np.random.seed(seed)  # Numpy module.\n",
        "random.seed(seed)  # Python random module.\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms(False)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVKuIzeTu30a",
        "outputId": "2b61ce9a-4de4-4ba9-916a-68470172b0e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DR6UKSGu30a"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRIiL-Hnu30b"
      },
      "outputs": [],
      "source": [
        "train_corpus_path = r\"PHOENIX-2014-T.train.corpus.csv\"\n",
        "dev_corpus_path = r\"PHOENIX-2014-T.dev.corpus.csv\"\n",
        "test_corpus_path = r\"PHOENIX-2014-T.test.corpus.csv\"\n",
        "\n",
        "train_videos_path = r\"D:\\phoenix-2014-T.v3\\PHOENIX-2014-T-release-v3\\PHOENIX-2014-T\\features\\fullFrame-210x260px\\train\"\n",
        "dev_videos_path = r\"D:\\phoenix-2014-T.v3\\PHOENIX-2014-T-release-v3\\PHOENIX-2014-T\\features\\fullFrame-210x260px\\dev\"\n",
        "test_videos_path = r\"D:\\phoenix-2014-T.v3\\PHOENIX-2014-T-release-v3\\PHOENIX-2014-T\\features\\fullFrame-210x260px\\test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9srErXlPu30b"
      },
      "source": [
        "['name', 'video', 'start', 'end', 'speaker', 'orth', 'translation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkhdQMmFu30c"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "                                    transforms.ToTensor()\n",
        "                                    ])\n",
        "\n",
        "train_data = PHEONIX14T(train_corpus_path, train_videos_path, data_transform, 300,)\n",
        "dev_data = PHEONIX14T(dev_corpus_path, dev_videos_path, data_transform, 300)\n",
        "test_data = PHEONIX14T(test_corpus_path, test_videos_path, data_transform, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fI-2gaSu30c",
        "outputId": "986b7dbe-9294-492a-ccd3-1a071abb4555"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[[[0.83434874, 0.81956189, 0.83206517, ..., 0.83039216,\n",
              "           0.8079066 , 0.84642857],\n",
              "          [0.82731092, 0.80513065, 0.82280725, ..., 0.82959449,\n",
              "           0.80705313, 0.83732493],\n",
              "          [0.83606443, 0.81547072, 0.82193737, ..., 0.82865896,\n",
              "           0.80807073, 0.83294818],\n",
              "          ...,\n",
              "          [0.87146359, 0.86505383, 0.86439076, ..., 0.88063725,\n",
              "           0.85850293, 0.90042017],\n",
              "          [0.78848039, 0.76065192, 0.74772847, ..., 0.85221573,\n",
              "           0.83303571, 0.87923669],\n",
              "          [0.71453081, 0.69206495, 0.67954854, ..., 0.82505252,\n",
              "           0.80734638, 0.86008403]],\n",
              " \n",
              "         [[0.86540616, 0.85061931, 0.87027202, ..., 0.86176471,\n",
              "           0.83927915, 0.87780112],\n",
              "          [0.8547619 , 0.83258163, 0.86349352, ..., 0.86096704,\n",
              "           0.83842568, 0.86869748],\n",
              "          [0.86509104, 0.84449733, 0.85888152, ..., 0.85845588,\n",
              "           0.83786765, 0.8627451 ],\n",
              "          ...,\n",
              "          [0.86995798, 0.86354823, 0.87544096, ..., 0.91670168,\n",
              "           0.89456736, 0.93648459],\n",
              "          [0.77279412, 0.74496564, 0.74858631, ..., 0.89143142,\n",
              "           0.8722514 , 0.91845238],\n",
              "          [0.70605742, 0.68359156, 0.68457633, ..., 0.87148109,\n",
              "           0.85377495, 0.90651261]],\n",
              " \n",
              "         [[0.88785014, 0.87306329, 0.86757484, ..., 0.93172269,\n",
              "           0.90923713, 0.9477591 ],\n",
              "          [0.90966387, 0.88748359, 0.90846901, ..., 0.92371214,\n",
              "           0.90117078, 0.93144258],\n",
              "          [0.92314426, 0.90255055, 0.91099659, ..., 0.91489846,\n",
              "           0.89431022, 0.91918768],\n",
              "          ...,\n",
              "          [0.85430672, 0.84789697, 0.85051317, ..., 0.94180672,\n",
              "           0.9196724 , 0.96158964],\n",
              "          [0.74926471, 0.72143623, 0.71843925, ..., 0.91496083,\n",
              "           0.89578081, 0.94198179],\n",
              "          [0.6897409 , 0.66727504, 0.62816986, ..., 0.90222339,\n",
              "           0.88451724, 0.9372549 ]]],\n",
              " \n",
              " \n",
              "        [[[0.83529412, 0.82107843, 0.81709668, ..., 0.83039216,\n",
              "           0.8079066 , 0.84642857],\n",
              "          [0.8362395 , 0.82202381, 0.82557554, ..., 0.82959449,\n",
              "           0.80705313, 0.83732493],\n",
              "          [0.83764006, 0.82770811, 0.8240218 , ..., 0.82865896,\n",
              "           0.80807073, 0.83294818],\n",
              "          ...,\n",
              "          [0.87461485, 0.87106092, 0.86537553, ..., 0.88063725,\n",
              "           0.85850293, 0.90042017],\n",
              "          [0.78848039, 0.76065192, 0.74772847, ..., 0.85221573,\n",
              "           0.83303571, 0.87923669],\n",
              "          [0.71453081, 0.69206495, 0.67954854, ..., 0.82505252,\n",
              "           0.80734638, 0.86008403]],\n",
              " \n",
              "         [[0.86698179, 0.85276611, 0.855402  , ..., 0.86176471,\n",
              "           0.83927915, 0.87780112],\n",
              "          [0.87153361, 0.85731793, 0.86748731, ..., 0.86096704,\n",
              "           0.83842568, 0.86869748],\n",
              "          [0.87135854, 0.8614266 , 0.86169905, ..., 0.85845588,\n",
              "           0.83786765, 0.8627451 ],\n",
              "          ...,\n",
              "          [0.87310924, 0.86955532, 0.87642573, ..., 0.91670168,\n",
              "           0.89456736, 0.93648459],\n",
              "          [0.77279412, 0.74496564, 0.74858631, ..., 0.89143142,\n",
              "           0.8722514 , 0.91845238],\n",
              "          [0.70605742, 0.68359156, 0.68457633, ..., 0.87148109,\n",
              "           0.85377495, 0.90651261]],\n",
              " \n",
              "         [[0.88911064, 0.87489496, 0.85265559, ..., 0.93172269,\n",
              "           0.90923713, 0.9477591 ],\n",
              "          [0.92251401, 0.90829832, 0.91185005, ..., 0.92371214,\n",
              "           0.90117078, 0.93144258],\n",
              "          [0.92706583, 0.91713388, 0.91344757, ..., 0.91489846,\n",
              "           0.89431022, 0.91918768],\n",
              "          ...,\n",
              "          [0.85745798, 0.85390406, 0.85149794, ..., 0.94180672,\n",
              "           0.9196724 , 0.96158964],\n",
              "          [0.74926471, 0.72143623, 0.71843925, ..., 0.91496083,\n",
              "           0.89578081, 0.94198179],\n",
              "          [0.6897409 , 0.66727504, 0.62816986, ..., 0.90222339,\n",
              "           0.88451724, 0.9372549 ]]],\n",
              " \n",
              " \n",
              "        [[[0.83529412, 0.81454175, 0.81596967, ..., 0.83039216,\n",
              "           0.8079066 , 0.84642857],\n",
              "          [0.8362395 , 0.82202381, 0.82557554, ..., 0.82959449,\n",
              "           0.80705313, 0.83732493],\n",
              "          [0.83764006, 0.82770811, 0.8240218 , ..., 0.82865896,\n",
              "           0.80807073, 0.83294818],\n",
              "          ...,\n",
              "          [0.87303922, 0.86662946, 0.86463695, ..., 0.88063725,\n",
              "           0.85850293, 0.90042017],\n",
              "          [0.78848039, 0.76065192, 0.74772847, ..., 0.85221573,\n",
              "           0.83303571, 0.87923669],\n",
              "          [0.71453081, 0.69206495, 0.67954854, ..., 0.82505252,\n",
              "           0.80734638, 0.86008403]],\n",
              " \n",
              "         [[0.86698179, 0.84622943, 0.85427499, ..., 0.86176471,\n",
              "           0.83927915, 0.87780112],\n",
              "          [0.87153361, 0.85731793, 0.86748731, ..., 0.86096704,\n",
              "           0.83842568, 0.86869748],\n",
              "          [0.87135854, 0.8614266 , 0.86169905, ..., 0.85845588,\n",
              "           0.83786765, 0.8627451 ],\n",
              "          ...,\n",
              "          [0.87153361, 0.86512386, 0.87568715, ..., 0.91670168,\n",
              "           0.89456736, 0.93648459],\n",
              "          [0.77279412, 0.74496564, 0.74858631, ..., 0.89143142,\n",
              "           0.8722514 , 0.91845238],\n",
              "          [0.70605742, 0.68359156, 0.68457633, ..., 0.87148109,\n",
              "           0.85377495, 0.90651261]],\n",
              " \n",
              "         [[0.88911064, 0.86835828, 0.85152858, ..., 0.93172269,\n",
              "           0.90923713, 0.9477591 ],\n",
              "          [0.92251401, 0.90829832, 0.91185005, ..., 0.92371214,\n",
              "           0.90117078, 0.93144258],\n",
              "          [0.92706583, 0.91713388, 0.91344757, ..., 0.91489846,\n",
              "           0.89431022, 0.91918768],\n",
              "          ...,\n",
              "          [0.85588235, 0.8494726 , 0.85075937, ..., 0.94180672,\n",
              "           0.9196724 , 0.96158964],\n",
              "          [0.74926471, 0.72143623, 0.71843925, ..., 0.91496083,\n",
              "           0.89578081, 0.94198179],\n",
              "          [0.6897409 , 0.66727504, 0.62816986, ..., 0.90222339,\n",
              "           0.88451724, 0.9372549 ]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[0.75402661, 0.73825608, 0.745783  , ..., 0.81609878,\n",
              "           0.79811034, 0.82384454],\n",
              "          [0.72251401, 0.7287651 , 0.80551471, ..., 0.81881237,\n",
              "           0.79182314, 0.8265056 ],\n",
              "          [0.76172969, 0.75671612, 0.80969997, ..., 0.82289916,\n",
              "           0.79326746, 0.82668067],\n",
              "          ...,\n",
              "          [0.85819328, 0.86815695, 0.84933473, ..., 0.87007616,\n",
              "           0.84622834, 0.8910014 ],\n",
              "          [0.74614846, 0.75055913, 0.76882988, ..., 0.84357493,\n",
              "           0.82547159, 0.87436975],\n",
              "          [0.64222689, 0.65231749, 0.70139837, ..., 0.82802871,\n",
              "           0.81001401, 0.85976891]],\n",
              " \n",
              "         [[0.90430672, 0.8885362 , 0.89668352, ..., 0.8661338 ,\n",
              "           0.84814535, 0.87387955],\n",
              "          [0.88721989, 0.89347098, 0.86433824, ..., 0.85802805,\n",
              "           0.83103882, 0.86572129],\n",
              "          [0.88389356, 0.87887999, 0.86187631, ..., 0.86053922,\n",
              "           0.83090752, 0.86432073],\n",
              "          ...,\n",
              "          [0.86446078, 0.87442446, 0.84904368, ..., 0.91163778,\n",
              "           0.88778996, 0.93256303],\n",
              "          [0.7539916 , 0.75840227, 0.75682007, ..., 0.88671218,\n",
              "           0.86860885, 0.917507  ],\n",
              "          [0.66088936, 0.67097995, 0.69412202, ..., 0.87116597,\n",
              "           0.85315126, 0.90290616]],\n",
              " \n",
              "         [[0.91526611, 0.89949558, 0.86043308, ..., 0.93248534,\n",
              "           0.91449689, 0.94023109],\n",
              "          [0.84408263, 0.85033373, 0.87745098, ..., 0.92077315,\n",
              "           0.89378392, 0.92846639],\n",
              "          [0.88014706, 0.87513349, 0.88380274, ..., 0.91855742,\n",
              "           0.88892573, 0.92233894],\n",
              "          ...,\n",
              "          [0.57811625, 0.58807992, 0.56851913, ..., 0.94374562,\n",
              "           0.9198978 , 0.96467087],\n",
              "          [0.30693277, 0.31134344, 0.34615831, ..., 0.92984944,\n",
              "           0.9117461 , 0.96064426],\n",
              "          [0.21022409, 0.22031469, 0.25551033, ..., 0.91430322,\n",
              "           0.89628852, 0.94604342]]],\n",
              " \n",
              " \n",
              "        [[[0.74667367, 0.72890406, 0.74733237, ..., 0.81609878,\n",
              "           0.79811034, 0.82384454],\n",
              "          [0.76092437, 0.74658176, 0.80796569, ..., 0.81881237,\n",
              "           0.79182314, 0.8265056 ],\n",
              "          [0.75689776, 0.75762758, 0.80993522, ..., 0.82289916,\n",
              "           0.79326746, 0.82668067],\n",
              "          ...,\n",
              "          [0.85276611, 0.85771621, 0.82584909, ..., 0.87007616,\n",
              "           0.84622834, 0.8910014 ],\n",
              "          [0.72058824, 0.72315848, 0.69798779, ..., 0.84357493,\n",
              "           0.82547159, 0.87436975],\n",
              "          [0.58953081, 0.57645746, 0.60089614, ..., 0.82802871,\n",
              "           0.81001401, 0.85976891]],\n",
              " \n",
              "         [[0.89145658, 0.87368697, 0.87303046, ..., 0.8661338 ,\n",
              "           0.84814535, 0.87387955],\n",
              "          [0.90210084, 0.88775823, 0.86311275, ..., 0.85802805,\n",
              "           0.83103882, 0.86572129],\n",
              "          [0.87759104, 0.87832086, 0.86188178, ..., 0.86053922,\n",
              "           0.83090752, 0.86432073],\n",
              "          ...,\n",
              "          [0.8769958 , 0.8819459 , 0.8600348 , ..., 0.91163778,\n",
              "           0.88778996, 0.93256303],\n",
              "          [0.76372549, 0.76629574, 0.74443387, ..., 0.88671218,\n",
              "           0.86860885, 0.917507  ],\n",
              "          [0.65430672, 0.64123337, 0.65680913, ..., 0.87116597,\n",
              "           0.85315126, 0.90290616]],\n",
              " \n",
              "         [[0.99369748, 0.99226956, 0.95785189, ..., 0.93248534,\n",
              "           0.91449689, 0.94023109],\n",
              "          [0.91778711, 0.9034445 , 0.89203431, ..., 0.92077315,\n",
              "           0.89378392, 0.92846639],\n",
              "          [0.90115546, 0.90188529, 0.89203431, ..., 0.91855742,\n",
              "           0.88892573, 0.92233894],\n",
              "          ...,\n",
              "          [0.60630252, 0.61125263, 0.61164653, ..., 0.94374562,\n",
              "           0.9198978 , 0.96467087],\n",
              "          [0.34019608, 0.34276633, 0.38708093, ..., 0.92984944,\n",
              "           0.9117461 , 0.96064426],\n",
              "          [0.19831933, 0.18524597, 0.25178353, ..., 0.91430322,\n",
              "           0.89628852, 0.94604342]]],\n",
              " \n",
              " \n",
              "        [[[0.7469888 , 0.7306471 , 0.71415551, ..., 0.81609878,\n",
              "           0.79811034, 0.82384454],\n",
              "          [0.76579132, 0.76836156, 0.81164216, ..., 0.81881237,\n",
              "           0.79182314, 0.8265056 ],\n",
              "          [0.76946779, 0.78012955, 0.81361169, ..., 0.82289916,\n",
              "           0.79326746, 0.82668067],\n",
              "          ...,\n",
              "          [0.817507  , 0.8124617 , 0.86680125, ..., 0.87007616,\n",
              "           0.84622834, 0.8910014 ],\n",
              "          [0.70721289, 0.71787465, 0.72568387, ..., 0.84357493,\n",
              "           0.82547159, 0.87436975],\n",
              "          [0.57839636, 0.58578978, 0.58549107, ..., 0.82802871,\n",
              "           0.81001401, 0.85976891]],\n",
              " \n",
              "         [[0.89677871, 0.88043702, 0.84341299, ..., 0.8661338 ,\n",
              "           0.84814535, 0.87387955],\n",
              "          [0.87951681, 0.88208705, 0.85919118, ..., 0.85802805,\n",
              "           0.83103882, 0.86572129],\n",
              "          [0.86113445, 0.87179622, 0.85771402, ..., 0.86053922,\n",
              "           0.83090752, 0.86432073],\n",
              "          ...,\n",
              "          [0.88648459, 0.88143929, 0.89808189, ..., 0.91163778,\n",
              "           0.88778996, 0.93256303],\n",
              "          [0.78564426, 0.79630602, 0.76110053, ..., 0.88671218,\n",
              "           0.86860885, 0.917507  ],\n",
              "          [0.67846639, 0.68585981, 0.63646052, ..., 0.87116597,\n",
              "           0.85315126, 0.90290616]],\n",
              " \n",
              "         [[0.99054622, 0.99054622, 0.97639509, ..., 0.93248534,\n",
              "           0.91449689, 0.94023109],\n",
              "          [0.87951681, 0.88208705, 0.89558824, ..., 0.92077315,\n",
              "           0.89378392, 0.92846639],\n",
              "          [0.88319328, 0.89385504, 0.89755777, ..., 0.91855742,\n",
              "           0.88892573, 0.92233894],\n",
              "          ...,\n",
              "          [0.58676471, 0.58171941, 0.65307576, ..., 0.94374562,\n",
              "           0.9198978 , 0.96467087],\n",
              "          [0.33466387, 0.34532563, 0.41269367, ..., 0.92984944,\n",
              "           0.9117461 , 0.96064426],\n",
              "          [0.2022409 , 0.20963432, 0.2323792 , ..., 0.91430322,\n",
              "           0.89628852, 0.94604342]]]]),\n",
              " 'ORT REGEN DURCH REGEN KOENNEN UEBERSCHWEMMUNG KOENNEN',\n",
              " 'mancherorts regnet es auch lÃ¤nger und ergiebig auch lokale Ã¼berschwemmungen sind wieder mÃ¶glich')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.__getitem__(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DojY-0Dhu30c"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=2,\n",
        "                              collate_fn=collate_fn,\n",
        "                              shuffle=True,\n",
        "                              worker_init_fn=seed_worker)\n",
        "\n",
        "dev_dataloader = DataLoader(dataset=dev_data,\n",
        "                             batch_size=2,\n",
        "                             collate_fn=collate_fn,\n",
        "                             shuffle=False,\n",
        "                             worker_init_fn=seed_worker)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=2,\n",
        "                             collate_fn=collate_fn,\n",
        "                             shuffle=False,\n",
        "                             worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhlgQKiu30d",
        "outputId": "09ce3f52-8902-4152-a3b2-9f0026226163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3548"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS5RGuzbu30d",
        "outputId": "620058ff-44ba-49eb-a5a3-5cd3abf4c03d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dev_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfRDpOQou30d"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbOWJuQru30d"
      },
      "outputs": [],
      "source": [
        "def extract_features(video_batch, window_size, stride, extractor, preprocess):\n",
        "    features = []\n",
        "    batch_size, max_len, H, W, C = video_batch.shape\n",
        "    # Calculate number of clips (Tf) based on window size and stride\n",
        "    Tf = (max_len - window_size) // stride + 1\n",
        "\n",
        "    for i in range(batch_size):  # Loop over each video in the batch\n",
        "        video_clips = []\n",
        "        for j in range(0, max_len-window_size+1, stride):  # Loop over time dimension to create clips\n",
        "            clip = video_batch[i, j:j+window_size]  # Get a clip\n",
        "            clip = clip.permute(0, 3, 1, 2)  # Reshape clip to [T, C, H, W]\n",
        "            clip = torch.stack([preprocess(frame) for frame in clip])  # Apply preprocessing to each frame\n",
        "            video_clips.append(clip)\n",
        "\n",
        "        # Convert list of tensors to a tensor\n",
        "        video_clips = torch.stack(video_clips)\n",
        "\n",
        "        # Pass each clip through the extractor\n",
        "        clip_features = [extractor(clip) for clip in video_clips]\n",
        "        # Combine features from all clips (stacking and reshaping as necessary)\n",
        "        clip_features = torch.stack(clip_features)\n",
        "        features.append(clip_features)\n",
        "\n",
        "    # Combine features for the entire batch\n",
        "    features = torch.stack(features)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BFo6Vv3u30e"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySInIABhu30e"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder layer.\n",
        "\n",
        "    Consists of self-attention, source-attention, and feed-forward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, size: int = 0, ff_size: int = 0, num_heads: int = 0, dropout: float = 0.1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a single Transformer decoder layer.\n",
        "\n",
        "        It attends to the source representation and the previous decoder states.\n",
        "\n",
        "        :param size: model dimensionality\n",
        "        :param ff_size: size of the feed-forward intermediate layer\n",
        "        :param num_heads: number of heads\n",
        "        :param dropout: dropout to apply to input\n",
        "        \"\"\"\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "\n",
        "        self.trg_trg_att = nn.MultiheadAttention(size, num_heads,batch_first=True)\n",
        "        self.src_trg_att = nn.MultiheadAttention(400, num_heads,batch_first=True)\n",
        "\n",
        "        self.feed_forward = PositionWiseFeedForward(size, ff_size)\n",
        "        # self.projection = nn.Linear(size, 400)\n",
        "        self.x_layer_norm = nn.LayerNorm(size, eps=1e-6)\n",
        "        self.dec_layer_norm = nn.LayerNorm(size, eps=1e-6)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # pylint: disable=arguments-differ\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        memory,\n",
        "        src_mask,\n",
        "        trg_mask,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass of a single Transformer decoder layer.\n",
        "\n",
        "        :param x: inputs\n",
        "        :param memory: source representations\n",
        "        :param src_mask: source mask\n",
        "        :param trg_mask: target mask (so as to not condition on future steps)\n",
        "        :return: output tensor\n",
        "        \"\"\"\n",
        "        # decoder/target self-attention\n",
        "        x_norm = self.x_layer_norm(x)\n",
        "        h1, scores1 = self.trg_trg_att(x_norm, x_norm, x_norm, attn_mask =trg_mask)\n",
        "\n",
        "        h1 = self.dropout(h1) + x\n",
        "\n",
        "        # source-target attention\n",
        "        h1_norm = self.dec_layer_norm(h1)\n",
        "        # h1_projected = self.projection(h1_norm)\n",
        "        h2, scores2 = self.src_trg_att(memory, memory, h1_norm, src_mask)\n",
        "\n",
        "        # final position-wise feed-forward layer\n",
        "        o = self.feed_forward(self.dropout(h2) + h1)\n",
        "\n",
        "        return o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb95ux5Gu30e"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A transformer decoder with N masked layers.\n",
        "    Decoder layers are masked so that an attention head cannot see the future.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 3,\n",
        "        num_heads: int = 8,\n",
        "        hidden_size: int = 400,\n",
        "        ff_size: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "        emb_dropout: float = 0.1,\n",
        "        vocab_size: int = 31102,\n",
        "        freeze: bool = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a Transformer decoder.\n",
        "\n",
        "        :param num_layers: number of Transformer layers\n",
        "        :param num_heads: number of heads for each layer\n",
        "        :param hidden_size: hidden size\n",
        "        :param ff_size: position-wise feed-forward size\n",
        "        :param dropout: dropout probability (1-keep)\n",
        "        :param emb_dropout: dropout probability for embeddings\n",
        "        :param vocab_size: size of the output vocabulary\n",
        "        :param freeze: set to True keep all decoder parameters fixed\n",
        "        :param kwargs:\n",
        "        \"\"\"\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self._hidden_size = hidden_size\n",
        "        self._output_size = vocab_size\n",
        "\n",
        "        # create num_layers decoder layers and put them in a list\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderLayer(\n",
        "                    size=hidden_size,\n",
        "                    ff_size=ff_size,\n",
        "                    num_heads=num_heads,\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.pe = PositionalEncoding(hidden_size, 31102)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(p=emb_dropout)\n",
        "        self.output_layer = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        trg_embed= None,\n",
        "        encoder_output= None,\n",
        "        encoder_hidden= None,\n",
        "        src_mask= None,\n",
        "        unroll_steps: int = None,\n",
        "        hidden= None,\n",
        "        trg_mask= None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Transformer decoder forward pass.\n",
        "\n",
        "        :param trg_embed: embedded targets\n",
        "        :param encoder_output: source representations\n",
        "        :param encoder_hidden: unused\n",
        "        :param src_mask:\n",
        "        :param unroll_steps: unused\n",
        "        :param hidden: unused\n",
        "        :param trg_mask: to mask out target paddings\n",
        "                         Note that a subsequent mask is applied here.\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        assert trg_mask is not None, \"trg_mask required for Transformer\"\n",
        "\n",
        "        x = self.pe(trg_embed)  # add position encoding to word embedding\n",
        "        x = self.emb_dropout(x)\n",
        "\n",
        "        # trg_mask = trg_mask & subsequent_mask(trg_embed.size(1)).type_as(trg_mask)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x=x, memory=encoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output, x, None, None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"%s(num_layers=%r, num_heads=%r)\" % (\n",
        "            self.__class__.__name__,\n",
        "            len(self.layers),\n",
        "            self.layers[0].trg_trg_att.num_heads,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXt-tfZ-u30e"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUXNKQBTu30e"
      },
      "outputs": [],
      "source": [
        "class InitializationModule(nn.Module):\n",
        "    def __init__(self,\n",
        "                hidden_size = 400,\n",
        "                ff_size = 2048,\n",
        "                num_layers = 3,\n",
        "                num_heads = 8,\n",
        "                dropout= 0.1):\n",
        "        super(InitializationModule, self).__init__()\n",
        "        self.encoder = TransformerEncoder()\n",
        "        self.decoder = TransformerDecoder()\n",
        "\n",
        "    def forward(self, video, translation,src_mask, tgt_mask):\n",
        "\n",
        "        encoded = self.encoder(video, src_mask, initial = True)\n",
        "        decoded = self.decoder(translation, encoded, src_mask=src_mask, trg_mask=tgt_mask)\n",
        "\n",
        "        return encoded, decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRWGC-LCu30e"
      },
      "outputs": [],
      "source": [
        "class IterativePrototypeRefinement(nn.Module):\n",
        "    def __init__(self,\n",
        "                hidden_size = 400,\n",
        "                ff_size = 2048,\n",
        "                num_layers = 3,\n",
        "                num_heads = 8,\n",
        "                dropout= 0.1,\n",
        "                K = 3):\n",
        "        super(IterativePrototypeRefinement, self).__init__()\n",
        "        self.encoder = TransformerEncoder()\n",
        "        self.decoder = TransformerDecoder()\n",
        "        self.K = 3\n",
        "\n",
        "    def forward(self, video, encoded_mem, translation,src_mask, tgt_mask):\n",
        "\n",
        "        decoder_outputs = []\n",
        "        for i in range(self.K):\n",
        "            encoded = self.encoder(video, src_mask, encoded_mem)\n",
        "            decoded = self.decoder(video, encoded, src_mask=src_mask, trg_mask=tgt_mask)\n",
        "            decoder_outputs.append(decoded[0].detach().cpu())\n",
        "        return decoder_outputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vZ5VeiHu30f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    # Create a matrix where the entries above the diagonal are True (masked)\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1).bool()\n",
        "    return mask\n",
        "\n",
        "def create_trg_mask(trg, pad_token_id):\n",
        "    # trg: [batch_size, trg_len]\n",
        "    # pad_token_id: the token ID used for padding (e.g., BERT's [PAD] token ID)\n",
        "\n",
        "    # Create a padding mask for ignoring pad tokens\n",
        "    pad_mask = (trg == pad_token_id).unsqueeze(1)  # Shape: [batch_size, 1, trg_len]\n",
        "\n",
        "    # Create the look-ahead mask\n",
        "    trg_len = trg.size(1)\n",
        "    look_ahead_mask = create_look_ahead_mask(trg_len)  # Shape: [trg_len, trg_len]\n",
        "    look_ahead_mask = look_ahead_mask.to(trg.device).expand(trg.size(0), trg_len, trg_len)  # [batch_size, trg_len, trg_len]\n",
        "\n",
        "    # Combine the masks\n",
        "    trg_mask = pad_mask | look_ahead_mask  # Shape: [batch_size, trg_len, trg_len]\n",
        "    return trg_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yslt5vmGu30f"
      },
      "outputs": [],
      "source": [
        "def get_batch_video_clips( video_batch, window_size, stride):\n",
        "        batch_video_clips = []\n",
        "        batch_size, max_len, H, W, C = video_batch.shape\n",
        "        # Calculate number of clips (Tf) based on window size and stride\n",
        "        Tf = (max_len - window_size) // stride + 1\n",
        "\n",
        "        for i in range(batch_size):  # Loop over each video in the batch\n",
        "            video_clips = []\n",
        "            for j in range(0, max_len-window_size+1, stride):  # Loop over time dimension to create clips\n",
        "                clip = video_batch[i, j:j+window_size]  # Get a clip\n",
        "                clip = clip.permute(0, 3, 1, 2)  # Reshape clip to [T, C, H, W]\n",
        "                video_clips.append(clip)\n",
        "\n",
        "            # Convert list of tensors to a tensor\n",
        "            video_clips = torch.stack(video_clips)\n",
        "            batch_video_clips.append(video_clips)\n",
        "\n",
        "        # Combine features for the entire batch\n",
        "        batch_video_clips = torch.stack(batch_video_clips)\n",
        "        return batch_video_clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq7OAYcBu30f"
      },
      "outputs": [],
      "source": [
        "class IPSLT(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                hidden_size = 400,\n",
        "                ff_size = 2048,\n",
        "                num_layers = 3,\n",
        "                num_heads = 8,\n",
        "                dropout= 0.1, K = 3):\n",
        "        super(IPSLT, self).__init__()\n",
        "        self.initalization_module = InitializationModule()\n",
        "        self.iterative_prototype_refinement = IterativePrototypeRefinement()\n",
        "        # Parameters\n",
        "        vocab_size = 1000  # Example vocabulary size\n",
        "        embedding_dim = 400  # Number of features per embedding\n",
        "\n",
        "        # Creating the embedding layer\n",
        "        self.embedding_layer = nn.Embedding(31102, embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, batch_video_clips, translation):\n",
        "\n",
        "        features_video_clips = []\n",
        "        # Create a tensor of zeros with the same batch size (2) but with 25 additional features\n",
        "        zeros_to_add = torch.zeros(translation.size(0), max(70 -translation.size(1), 0), dtype=torch.int64)\n",
        "\n",
        "        # Concatenate the original tensor with the zeros tensor along the second dimension\n",
        "        translation = torch.cat((translation, zeros_to_add), dim=1)\n",
        "        embeddings = self.embedding_layer(translation.to(device))\n",
        "        # with torch.no_grad():\n",
        "        #     output = bert_model(translation)\n",
        "        # embeddings = output.last_hidden_state  # (2, 42, 768)\n",
        "\n",
        "        for clip_index in range(batch_video_clips.shape[1]):\n",
        "            temp_batch_video_clips = batch_video_clips[:, clip_index].squeeze(1).permute(0, 2, 1, 3, 4).to(device)\n",
        "            features = i3d(temp_batch_video_clips)\n",
        "            features_video_clips.append(features.detach().cpu())\n",
        "\n",
        "        features_video_clips =  torch.stack(features_video_clips, dim=1)\n",
        "        required_shape = (2, 70, 400)\n",
        "        num_new_rows = required_shape[1] - features_video_clips.shape[1]\n",
        "\n",
        "        # Create a tensor of zeros with the appropriate shape\n",
        "        zeros_to_add = torch.zeros(2, num_new_rows, 400)\n",
        "\n",
        "        # Concatenate the original tensor with the zeros tensor\n",
        "        features_video_clips = torch.cat((features_video_clips.to(\"cpu\"), zeros_to_add), dim=1)\n",
        "        sgn_mask = (features_video_clips != torch.zeros(400))[..., 0].to(device)#.unsqueeze(1)\n",
        "        print(sgn_mask.shape)\n",
        "        print(features_video_clips.shape)\n",
        "        trg_mask = create_trg_mask(translation, 0).to(device)\n",
        "\n",
        "\n",
        "        # # txt_input is used for teacher forcing, last one is cut off\n",
        "        # txt_input = translation[:, :-1]\n",
        "        # # txt is used for loss computation, shifted by one since BOS\n",
        "        # txt = translation[:, 1:]\n",
        "        # # we exclude the padded areas from the loss computation\n",
        "        # txt_mask = (self.txt_input != 0).unsqueeze(1)\n",
        "        # num_txt_tokens = (self.txt != 0).data.sum().item()\n",
        "        features_video_clips = features_video_clips.to(device)\n",
        "\n",
        "        decoder_outputs_new = []\n",
        "        encoded, decoder_output = self.initalization_module(features_video_clips, embeddings, sgn_mask, trg_mask.repeat(8, 1, 1))\n",
        "        decoder_outputs_new.append(decoder_output[0].cpu())\n",
        "        decoder_outputs = self.iterative_prototype_refinement(features_video_clips, encoded, embeddings, sgn_mask, trg_mask.repeat(8, 1, 1))\n",
        "        decoder_outputs_new.extend(decoder_outputs)\n",
        "        features_video_clips = features_video_clips.detach().cpu()\n",
        "        del features_video_clips\n",
        "        return decoder_outputs_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCn4kq8Du30f"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26D6qKHZu30g"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(filepath, model, optimizer):\n",
        "    \"\"\"\n",
        "    Load a model checkpoint and restore the model and optimizer states.\n",
        "\n",
        "    Args:\n",
        "    filepath (str): Path to the checkpoint file.\n",
        "    model (torch.nn.Module): Model to load the state_dict into.\n",
        "    optimizer (torch.optim.Optimizer): Optimizer to load the state_dict into.\n",
        "\n",
        "    Returns:\n",
        "    int: The epoch at which the checkpoint was saved.\n",
        "    float: The loss at the checkpoint.\n",
        "    \"\"\"\n",
        "    # Load the saved file\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    # Restore the model and optimizer state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Return the epoch and loss from the checkpoint\n",
        "    return checkpoint['epoch'], checkpoint['loss']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72CAMH8tu30g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYNYdwBru30g"
      },
      "outputs": [],
      "source": [
        "model = IPSLT()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "epochs = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGGyxqR5u30g"
      },
      "outputs": [],
      "source": [
        "# epoch, loss = load_checkpoint('best_model_checkpoint.pth', model, optimizer)\n",
        "# print(f\"Loaded checkpoint from epoch {epoch} with loss {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzF3bDWSu30g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def custom_loss_function(translation, decoder_outputs, K=3):\n",
        "\n",
        "    logits = decoder_outputs[0].view(-1, decoder_outputs[0].shape[-1]).to(\"cpu\")  # Shape: [batch_size * sequence_length, vocab_size]\n",
        "    zeros_to_add = torch.zeros(translation.size(0), max(70 -translation.size(1), 0), dtype=torch.int64)\n",
        "    translation = torch.cat((translation, zeros_to_add), dim=1)\n",
        "    target_ids = translation.view(-1)\n",
        "\n",
        "    token_ids = torch.argmax(decoder_outputs[0], dim=-1)\n",
        "\n",
        "    # # Convert token IDs to sentences\n",
        "    # sentences = []\n",
        "    # for i in range(token_ids.size(0)):\n",
        "    #     tokens = tokenizer.convert_ids_to_tokens(token_ids[i])\n",
        "    #     sentence = tokenizer.convert_tokens_to_string(tokens)\n",
        "    #     sentences.append(sentence)\n",
        "\n",
        "    # # Print the sentences\n",
        "    # for i, sentence in enumerate(sentences):\n",
        "    #     print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n",
        "    # Calculate the cross entropy loss\n",
        "    ce_loss = F.cross_entropy(logits, target_ids, ignore_index = 0)\n",
        "    for decoder_output in decoder_outputs:\n",
        "\n",
        "        ce_loss_2 = F.cross_entropy(decoder_output.view(-1, decoder_output.shape[-1]) , target_ids, ignore_index = 0)\n",
        "\n",
        "    # Calculate the iterative distillation loss\n",
        "    idl_loss = 0.0\n",
        "    final_logits = decoder_outputs[-1].detach().view(-1, decoder_outputs[-1].shape[-1])  # Get the logits of the final output\n",
        "    final_probs = F.softmax(final_logits, dim=1)  # Calculate the softmax probabilities of the final logits\n",
        "\n",
        "    for i, decoder_output in enumerate(decoder_outputs[:-1]):  # Iterate over all but the last decoder output\n",
        "        logits = decoder_output.view(-1, decoder_output.shape[-1])  # Flatten the output to [batch_size * sequence_length, vocab_size]\n",
        "        probs = F.log_softmax(logits, dim=1)  # Compute log softmax probabilities\n",
        "        idl_loss += F.kl_div(probs, final_probs, reduction='batchmean')  # Compute KL divergence\n",
        "\n",
        "        # To ignore specific indices (e.g., index 0 for padding), you can apply a mask\n",
        "        mask = (logits.argmax(dim=1) != 0)  # Create a mask where the maximum index is not 0\n",
        "        idl_loss *= mask.float().mean()  # Apply the mask to the loss (adjust based on your specific needs)\n",
        "     # Combine the losses\n",
        "    combined_loss = ce_loss + 15 * idl_loss + ce_loss_2\n",
        "\n",
        "    return combined_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95dKLSvru30g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def custom_loss_function(translation, decoder_outputs, K=3):\n",
        "    # Flatten logits and convert to CPU for loss computation\n",
        "    logits = decoder_outputs[0].view(-1, decoder_outputs[0].shape[-1]).to(\"cpu\")\n",
        "\n",
        "    # Padding translation to match the maximum length (70)\n",
        "    zeros_to_add = torch.zeros(translation.size(0), max(70 - translation.size(1), 0), dtype=torch.int64)\n",
        "    translation = torch.cat((translation, zeros_to_add), dim=1)\n",
        "    target_ids = translation.view(-1)\n",
        "\n",
        "    # Calculate the cross-entropy loss for the first decoder output\n",
        "    ce_loss = F.cross_entropy(logits, target_ids, ignore_index=0)\n",
        "\n",
        "    # Calculate the cross-entropy loss for each decoder output and sum them up\n",
        "    ce_loss_2 = 0.0\n",
        "    for decoder_output in decoder_outputs:\n",
        "        ce_loss_2 += F.cross_entropy(decoder_output.view(-1, decoder_output.shape[-1]), target_ids, ignore_index=0)\n",
        "\n",
        "    # Calculate the iterative distillation loss (IDL)\n",
        "    idl_loss = 0.0\n",
        "    final_logits = decoder_outputs[-1].detach().view(-1, decoder_outputs[-1].shape[-1])\n",
        "    final_probs = F.softmax(final_logits, dim=1)\n",
        "\n",
        "    for i, decoder_output in enumerate(decoder_outputs[:-1]):\n",
        "        logits = decoder_output.view(-1, decoder_output.shape[-1])\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        idl_loss += F.kl_div(log_probs, final_probs, reduction='batchmean')\n",
        "\n",
        "        # Apply mask to ignore specific indices (e.g., index 0 for padding)\n",
        "        mask = (logits.argmax(dim=1) != 0)\n",
        "        idl_loss *= mask.float().mean()\n",
        "\n",
        "    # Combine the losses with weights summing to 1\n",
        "    weight_ce = 0.4\n",
        "    weight_idl = 0.2\n",
        "    weight_ce2 = 0.4\n",
        "\n",
        "    combined_loss = weight_ce * ce_loss + weight_idl * idl_loss + weight_ce2 * ce_loss_2\n",
        "\n",
        "    return combined_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoHrevQAu30g"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Saves a model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        state (dict): Contains model's state_dict, may include optimizer's state_dict\n",
        "        filename (str): Path to save the checkpoint\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGmpqjZGu30l",
        "outputId": "b3a82d6b-7373-4e6d-ac6a-6e5ffa9b2750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Batch Loss: 21.0273\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n",
            "torch.Size([2, 70])\n",
            "torch.Size([2, 70, 400])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Training mode\u001b[39;00m\n\u001b[0;32m     11\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (imgs, translation) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     13\u001b[0m     batch_video_clips \u001b[38;5;241m=\u001b[39m get_batch_video_clips(imgs, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     translation \u001b[38;5;241m=\u001b[39m translation\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:676\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory\u001b[49m:\n\u001b[0;32m    677\u001b[0m     data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter('runs/weight_2_2_1')\n",
        "\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(0, epochs):\n",
        "    model.train()  # Training mode\n",
        "    total_train_loss = 0\n",
        "    for batch, (imgs, translation) in enumerate(train_dataloader):\n",
        "        batch_video_clips = get_batch_video_clips(imgs, 30, 20).to(\"cpu\")\n",
        "        translation = translation.to(\"cpu\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        decoder_outputs = model(batch_video_clips, translation)\n",
        "        loss = custom_loss_function(translation, decoder_outputs)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log training loss every 100 batches\n",
        "        if batch % 100 == 0:\n",
        "            writer.add_scalar('Training loss', loss.item(), epoch * len(train_dataloader) + batch)\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(average_train_loss)\n",
        "    writer.add_scalar('Average Training Loss', average_train_loss, epoch)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {average_train_loss:.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for val_batch, (val_imgs, val_translation) in enumerate(dev_dataloader):\n",
        "            if val_imgs.shape[1] < 30 or val_imgs.shape[0] < 2:\n",
        "                continue\n",
        "            batch_video_clips = get_batch_video_clips(val_imgs, 30, 20).to(\"cpu\")\n",
        "            val_translation = val_translation.to(\"cpu\")\n",
        "            val_decoder_outputs = model(batch_video_clips, val_translation)\n",
        "            val_loss = custom_loss_function(val_translation, val_decoder_outputs)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(dev_dataloader)\n",
        "    dev_losses.append(average_val_loss)\n",
        "    writer.add_scalar('Average Validation Loss', average_val_loss, epoch)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {average_val_loss:.4f}')\n",
        "\n",
        "    # Save checkpoint if the validation loss improved\n",
        "    if average_val_loss < best_val_loss:\n",
        "        best_val_loss = average_val_loss\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': best_val_loss,\n",
        "        }, \"best_model_checkpoint.pth\")\n",
        "\n",
        "print(f'Best Validation Loss: {best_val_loss:.4f}')\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZE0Qvdtu30l",
        "outputId": "a18a2643-b3c3-4a50-dd78-e686314c8684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 111, 224, 224, 3])\n",
            "torch.Size([2, 148, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 165, 224, 224, 3])\n",
            "torch.Size([2, 81, 224, 224, 3])\n",
            "torch.Size([2, 103, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 105, 224, 224, 3])\n",
            "torch.Size([2, 164, 224, 224, 3])\n",
            "torch.Size([2, 116, 224, 224, 3])\n",
            "torch.Size([2, 154, 224, 224, 3])\n",
            "torch.Size([2, 126, 224, 224, 3])\n",
            "torch.Size([2, 139, 224, 224, 3])\n",
            "torch.Size([2, 117, 224, 224, 3])\n",
            "torch.Size([2, 139, 224, 224, 3])\n",
            "torch.Size([2, 114, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 62, 224, 224, 3])\n",
            "torch.Size([2, 191, 224, 224, 3])\n",
            "torch.Size([2, 173, 224, 224, 3])\n",
            "torch.Size([2, 86, 224, 224, 3])\n",
            "torch.Size([2, 228, 224, 224, 3])\n",
            "torch.Size([2, 157, 224, 224, 3])\n",
            "torch.Size([2, 97, 224, 224, 3])\n",
            "torch.Size([2, 174, 224, 224, 3])\n",
            "torch.Size([2, 99, 224, 224, 3])\n",
            "torch.Size([2, 106, 224, 224, 3])\n",
            "torch.Size([2, 167, 224, 224, 3])\n",
            "torch.Size([2, 79, 224, 224, 3])\n",
            "torch.Size([2, 58, 224, 224, 3])\n",
            "torch.Size([2, 117, 224, 224, 3])\n",
            "torch.Size([2, 120, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 114, 224, 224, 3])\n",
            "torch.Size([2, 115, 224, 224, 3])\n",
            "torch.Size([2, 215, 224, 224, 3])\n",
            "torch.Size([2, 125, 224, 224, 3])\n",
            "torch.Size([2, 144, 224, 224, 3])\n",
            "torch.Size([2, 160, 224, 224, 3])\n",
            "torch.Size([2, 153, 224, 224, 3])\n",
            "torch.Size([2, 167, 224, 224, 3])\n",
            "torch.Size([2, 114, 224, 224, 3])\n",
            "torch.Size([2, 193, 224, 224, 3])\n",
            "torch.Size([2, 109, 224, 224, 3])\n",
            "torch.Size([2, 140, 224, 224, 3])\n",
            "torch.Size([2, 127, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 70, 224, 224, 3])\n",
            "torch.Size([2, 98, 224, 224, 3])\n",
            "torch.Size([2, 162, 224, 224, 3])\n",
            "torch.Size([2, 88, 224, 224, 3])\n",
            "torch.Size([2, 61, 224, 224, 3])\n",
            "torch.Size([2, 72, 224, 224, 3])\n",
            "torch.Size([2, 61, 224, 224, 3])\n",
            "torch.Size([2, 160, 224, 224, 3])\n",
            "torch.Size([2, 92, 224, 224, 3])\n",
            "torch.Size([2, 169, 224, 224, 3])\n",
            "torch.Size([2, 171, 224, 224, 3])\n",
            "torch.Size([2, 124, 224, 224, 3])\n",
            "torch.Size([2, 133, 224, 224, 3])\n",
            "torch.Size([2, 163, 224, 224, 3])\n",
            "torch.Size([2, 35, 224, 224, 3])\n",
            "torch.Size([2, 152, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 162, 224, 224, 3])\n",
            "torch.Size([2, 113, 224, 224, 3])\n",
            "torch.Size([2, 69, 224, 224, 3])\n",
            "torch.Size([2, 198, 224, 224, 3])\n",
            "torch.Size([2, 139, 224, 224, 3])\n",
            "torch.Size([2, 169, 224, 224, 3])\n",
            "torch.Size([2, 167, 224, 224, 3])\n",
            "torch.Size([2, 106, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 86, 224, 224, 3])\n",
            "torch.Size([2, 102, 224, 224, 3])\n",
            "torch.Size([2, 180, 224, 224, 3])\n",
            "torch.Size([2, 180, 224, 224, 3])\n",
            "torch.Size([2, 123, 224, 224, 3])\n",
            "torch.Size([2, 99, 224, 224, 3])\n",
            "torch.Size([2, 90, 224, 224, 3])\n",
            "torch.Size([2, 135, 224, 224, 3])\n",
            "torch.Size([2, 176, 224, 224, 3])\n",
            "torch.Size([2, 184, 224, 224, 3])\n",
            "torch.Size([2, 158, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 117, 224, 224, 3])\n",
            "torch.Size([2, 135, 224, 224, 3])\n",
            "torch.Size([2, 97, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 121, 224, 224, 3])\n",
            "torch.Size([2, 157, 224, 224, 3])\n",
            "torch.Size([2, 169, 224, 224, 3])\n",
            "torch.Size([2, 169, 224, 224, 3])\n",
            "torch.Size([2, 130, 224, 224, 3])\n",
            "torch.Size([2, 113, 224, 224, 3])\n",
            "torch.Size([2, 106, 224, 224, 3])\n",
            "torch.Size([2, 118, 224, 224, 3])\n",
            "torch.Size([2, 99, 224, 224, 3])\n",
            "torch.Size([2, 167, 224, 224, 3])\n",
            "torch.Size([2, 102, 224, 224, 3])\n",
            "torch.Size([2, 70, 224, 224, 3])\n",
            "torch.Size([2, 155, 224, 224, 3])\n",
            "torch.Size([2, 209, 224, 224, 3])\n",
            "torch.Size([2, 109, 224, 224, 3])\n",
            "torch.Size([2, 143, 224, 224, 3])\n",
            "torch.Size([2, 138, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 122, 224, 224, 3])\n",
            "torch.Size([2, 65, 224, 224, 3])\n",
            "torch.Size([2, 120, 224, 224, 3])\n",
            "torch.Size([2, 126, 224, 224, 3])\n",
            "torch.Size([2, 125, 224, 224, 3])\n",
            "torch.Size([2, 103, 224, 224, 3])\n",
            "torch.Size([2, 131, 224, 224, 3])\n",
            "torch.Size([2, 62, 224, 224, 3])\n",
            "torch.Size([2, 170, 224, 224, 3])\n",
            "torch.Size([2, 212, 224, 224, 3])\n",
            "torch.Size([2, 128, 224, 224, 3])\n",
            "torch.Size([2, 133, 224, 224, 3])\n",
            "torch.Size([2, 116, 224, 224, 3])\n",
            "torch.Size([2, 112, 224, 224, 3])\n",
            "torch.Size([2, 164, 224, 224, 3])\n",
            "torch.Size([2, 120, 224, 224, 3])\n",
            "torch.Size([2, 51, 224, 224, 3])\n",
            "torch.Size([2, 109, 224, 224, 3])\n",
            "torch.Size([2, 68, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 139, 224, 224, 3])\n",
            "torch.Size([2, 217, 224, 224, 3])\n",
            "torch.Size([2, 65, 224, 224, 3])\n",
            "torch.Size([2, 77, 224, 224, 3])\n",
            "torch.Size([2, 62, 224, 224, 3])\n",
            "torch.Size([2, 131, 224, 224, 3])\n",
            "torch.Size([2, 125, 224, 224, 3])\n",
            "torch.Size([2, 113, 224, 224, 3])\n",
            "torch.Size([2, 118, 224, 224, 3])\n",
            "torch.Size([2, 109, 224, 224, 3])\n",
            "torch.Size([2, 165, 224, 224, 3])\n",
            "torch.Size([2, 210, 224, 224, 3])\n",
            "torch.Size([2, 128, 224, 224, 3])\n",
            "torch.Size([2, 117, 224, 224, 3])\n",
            "torch.Size([2, 179, 224, 224, 3])\n",
            "torch.Size([2, 132, 224, 224, 3])\n",
            "torch.Size([2, 142, 224, 224, 3])\n",
            "torch.Size([2, 126, 224, 224, 3])\n",
            "torch.Size([2, 153, 224, 224, 3])\n",
            "torch.Size([2, 85, 224, 224, 3])\n",
            "torch.Size([2, 140, 224, 224, 3])\n",
            "torch.Size([2, 184, 224, 224, 3])\n",
            "torch.Size([2, 65, 224, 224, 3])\n",
            "torch.Size([2, 130, 224, 224, 3])\n",
            "torch.Size([2, 87, 224, 224, 3])\n",
            "torch.Size([2, 140, 224, 224, 3])\n",
            "torch.Size([2, 95, 224, 224, 3])\n",
            "torch.Size([2, 65, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 133, 224, 224, 3])\n",
            "torch.Size([2, 121, 224, 224, 3])\n",
            "torch.Size([2, 125, 224, 224, 3])\n",
            "torch.Size([2, 80, 224, 224, 3])\n",
            "torch.Size([2, 63, 224, 224, 3])\n",
            "torch.Size([2, 89, 224, 224, 3])\n",
            "torch.Size([2, 162, 224, 224, 3])\n",
            "torch.Size([2, 123, 224, 224, 3])\n",
            "torch.Size([2, 93, 224, 224, 3])\n",
            "torch.Size([2, 90, 224, 224, 3])\n",
            "torch.Size([2, 112, 224, 224, 3])\n",
            "torch.Size([2, 140, 224, 224, 3])\n",
            "torch.Size([2, 151, 224, 224, 3])\n",
            "torch.Size([2, 81, 224, 224, 3])\n",
            "torch.Size([2, 112, 224, 224, 3])\n",
            "torch.Size([2, 180, 224, 224, 3])\n",
            "torch.Size([2, 26, 224, 224, 3])\n",
            "torch.Size([2, 216, 224, 224, 3])\n",
            "torch.Size([2, 168, 224, 224, 3])\n",
            "torch.Size([2, 200, 224, 224, 3])\n",
            "torch.Size([2, 136, 224, 224, 3])\n",
            "torch.Size([2, 212, 224, 224, 3])\n",
            "torch.Size([2, 127, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 182, 224, 224, 3])\n",
            "torch.Size([2, 141, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 114, 224, 224, 3])\n",
            "torch.Size([2, 108, 224, 224, 3])\n",
            "torch.Size([2, 103, 224, 224, 3])\n",
            "torch.Size([2, 126, 224, 224, 3])\n",
            "torch.Size([2, 105, 224, 224, 3])\n",
            "torch.Size([2, 223, 224, 224, 3])\n",
            "torch.Size([2, 272, 224, 224, 3])\n",
            "torch.Size([2, 94, 224, 224, 3])\n",
            "torch.Size([2, 124, 224, 224, 3])\n",
            "torch.Size([2, 177, 224, 224, 3])\n",
            "torch.Size([2, 105, 224, 224, 3])\n",
            "torch.Size([2, 185, 224, 224, 3])\n",
            "torch.Size([2, 68, 224, 224, 3])\n",
            "torch.Size([2, 119, 224, 224, 3])\n",
            "torch.Size([2, 133, 224, 224, 3])\n",
            "torch.Size([2, 148, 224, 224, 3])\n",
            "torch.Size([2, 159, 224, 224, 3])\n",
            "torch.Size([2, 129, 224, 224, 3])\n",
            "torch.Size([2, 158, 224, 224, 3])\n",
            "torch.Size([2, 159, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 115, 224, 224, 3])\n",
            "torch.Size([2, 251, 224, 224, 3])\n",
            "torch.Size([2, 98, 224, 224, 3])\n",
            "torch.Size([2, 89, 224, 224, 3])\n",
            "torch.Size([2, 202, 224, 224, 3])\n",
            "torch.Size([2, 178, 224, 224, 3])\n",
            "torch.Size([2, 140, 224, 224, 3])\n",
            "torch.Size([2, 191, 224, 224, 3])\n",
            "torch.Size([2, 154, 224, 224, 3])\n",
            "torch.Size([2, 120, 224, 224, 3])\n",
            "torch.Size([2, 145, 224, 224, 3])\n",
            "torch.Size([2, 220, 224, 224, 3])\n",
            "torch.Size([2, 132, 224, 224, 3])\n",
            "torch.Size([2, 132, 224, 224, 3])\n",
            "torch.Size([2, 143, 224, 224, 3])\n",
            "torch.Size([2, 120, 224, 224, 3])\n",
            "torch.Size([2, 197, 224, 224, 3])\n",
            "torch.Size([2, 194, 224, 224, 3])\n",
            "torch.Size([2, 112, 224, 224, 3])\n",
            "torch.Size([2, 84, 224, 224, 3])\n",
            "torch.Size([2, 98, 224, 224, 3])\n",
            "torch.Size([2, 65, 224, 224, 3])\n",
            "torch.Size([2, 122, 224, 224, 3])\n",
            "torch.Size([2, 111, 224, 224, 3])\n",
            "torch.Size([2, 189, 224, 224, 3])\n",
            "torch.Size([2, 125, 224, 224, 3])\n",
            "torch.Size([2, 104, 224, 224, 3])\n",
            "torch.Size([2, 138, 224, 224, 3])\n",
            "torch.Size([2, 151, 224, 224, 3])\n",
            "torch.Size([2, 172, 224, 224, 3])\n",
            "torch.Size([2, 183, 224, 224, 3])\n",
            "torch.Size([2, 94, 224, 224, 3])\n",
            "torch.Size([2, 210, 224, 224, 3])\n",
            "torch.Size([2, 147, 224, 224, 3])\n",
            "torch.Size([2, 107, 224, 224, 3])\n",
            "torch.Size([2, 188, 224, 224, 3])\n",
            "torch.Size([2, 124, 224, 224, 3])\n",
            "torch.Size([2, 185, 224, 224, 3])\n",
            "torch.Size([2, 89, 224, 224, 3])\n",
            "torch.Size([2, 100, 224, 224, 3])\n",
            "torch.Size([2, 94, 224, 224, 3])\n",
            "torch.Size([2, 139, 224, 224, 3])\n",
            "torch.Size([2, 160, 224, 224, 3])\n",
            "torch.Size([2, 151, 224, 224, 3])\n",
            "torch.Size([2, 106, 224, 224, 3])\n",
            "torch.Size([2, 166, 224, 224, 3])\n",
            "torch.Size([2, 128, 224, 224, 3])\n",
            "torch.Size([2, 81, 224, 224, 3])\n",
            "torch.Size([2, 234, 224, 224, 3])\n",
            "torch.Size([2, 83, 224, 224, 3])\n",
            "torch.Size([2, 159, 224, 224, 3])\n",
            "torch.Size([2, 96, 224, 224, 3])\n",
            "torch.Size([2, 150, 224, 224, 3])\n",
            "torch.Size([2, 114, 224, 224, 3])\n",
            "torch.Size([2, 149, 224, 224, 3])\n",
            "torch.Size([1, 164, 224, 224, 3])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m batch_video_clips \u001b[38;5;241m=\u001b[39m get_batch_video_clips(val_imgs, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m val_translation \u001b[38;5;241m=\u001b[39m val_translation\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m val_decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_video_clips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_translation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m custom_loss_function(val_translation, val_decoder_outputs)\n\u001b[0;32m     12\u001b[0m total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[23], line 47\u001b[0m, in \u001b[0;36mIPSLT.forward\u001b[1;34m(self, batch_video_clips, translation)\u001b[0m\n\u001b[0;32m     44\u001b[0m zeros_to_add \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m, num_new_rows, \u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Concatenate the original tensor with the zeros tensor\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m features_video_clips \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_video_clips\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeros_to_add\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m sgn_mask \u001b[38;5;241m=\u001b[39m (features_video_clips \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m400\u001b[39m))[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;66;03m#.unsqueeze(1)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m create_trg_mask(translation, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "total_val_loss = 0\n",
        "with torch.no_grad():  # No gradients needed for validation, which saves memory and computations\n",
        "    for val_batch, (val_imgs, val_translation) in enumerate(dev_dataloader):\n",
        "        print(val_imgs.shape)\n",
        "        if val_imgs.shape[1] < 30 or val_imgs.shape[0] <2:\n",
        "            continue\n",
        "        batch_video_clips = get_batch_video_clips(val_imgs, 30, 20).to(\"cpu\")\n",
        "        val_translation = val_translation.to(\"cpu\")\n",
        "        val_decoder_outputs = model(batch_video_clips, val_translation)\n",
        "        val_loss = custom_loss_function(val_translation, val_decoder_outputs)\n",
        "        total_val_loss += val_loss.item()\n",
        "\n",
        "average_val_loss = total_val_loss / len(dev_dataloader)\n",
        "dev_losses.append(average_val_loss)\n",
        "print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {average_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlUhRZEqu30l",
        "outputId": "79cb9236-2025-402e-9edf-d9c7cc1d6045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 5, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] das bedeutet viele wolken und immer wieder zum teil kra¤ftige schauer und gewitter [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] tiefer luftdruck bestimmt in den na¤chsten tagen unser wetter [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 6, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] meist weht nur ein schwacher wind aus unterschiedlichen richtungen der bei schauern und gewittern stark ba [UNK] ig sein kann [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] am mittwoch hier und da nieselregen in der nordwestha¤lfte an den [UNK] kra¤ftiger wind [SEP]geborengeborengeborengeborengeborengeborenmalereigeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 4, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] und nun die wettervorhersage [UNK] morgen freitag den sechsten mai [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] am tag breiten sich die teilweise kra¤ftigen schneefa¤lle weiter aus [SEP]geborengeborengeborengeboren kingeborengeborengeborengeborengeboren kin kin kinmalerei kin kin kingeborengeborengeborengeborengeborenmalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 7, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] es wird wa¤rmer aus dem [UNK] morgen haben wir schon [UNK] frankreich die [UNK] bis zwanzig grad und am wochenende kommt diese wa¤rme langsam auch zu uns [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] morgen werte von zwanzig grad an der da¤ge grenze bis dreiayig grad am oberrhein [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereigeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 3, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] am tag nur hier und da einige sonnige momente vor allem an den alpen [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] jetzt [UNK] ich ihnen noch einen scha [UNK] nen abend [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 4, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] gelegentlich schneit es leicht richtung nordosten kann es auch noch kra¤ftiger schneien [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] auch im norden und westen wird es spa¤ter gebietsweise regnen [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 4, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] in der nacht [UNK] grad an der nordfriesischen [UNK] und minus vier grad an den alpen [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] am tag im [UNK] und in der mitte mitunter dichte nebelfelder [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 4, 30, 3, 224, 224])\n",
            "Sentence 1: [CLS] dabei erreicht der regen erst am freitag abend den [UNK] [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "Sentence 2: [CLS] in norddeutschland dagegen nur wolken und der frost ja den haben wir wieder in der [UNK] [SEP]geborengeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborenmalereimalereimalereimalereimalereimalereigeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeborengeboren\n",
            "torch.Size([2, 7, 30, 3, 224, 224])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[86], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m val_translation \u001b[38;5;241m=\u001b[39m val_translation\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_video_clips\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m val_decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_video_clips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_translation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m val_translation \u001b[38;5;241m=\u001b[39m val_translation\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m custom_loss_function(val_translation, val_decoder_outputs)\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Phoenix\\Desktop\\gflst\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[82], line 37\u001b[0m, in \u001b[0;36mIPSLT.forward\u001b[1;34m(self, batch_video_clips, translation)\u001b[0m\n\u001b[0;32m     35\u001b[0m     temp_batch_video_clips \u001b[38;5;241m=\u001b[39m batch_video_clips[:, clip_index]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m     features \u001b[38;5;241m=\u001b[39m i3d(temp_batch_video_clips)\n\u001b[1;32m---> 37\u001b[0m     features_video_clips\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     39\u001b[0m features_video_clips \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mstack(features_video_clips, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m required_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m400\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Validation phase\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "total_val_loss = 0\n",
        "with torch.no_grad():  # No gradients needed for validation, which saves memory and computations\n",
        "    for val_batch, (val_imgs, val_translation) in enumerate(dev_dataloader):\n",
        "        batch_video_clips = get_batch_video_clips(val_imgs, 20, 10) # [4, 5, 50, 3, 260, 210]\n",
        "        batch_video_clips = batch_video_clips.to(\"cpu\")\n",
        "        val_translation = val_translation.to(\"cpu\")\n",
        "        print(batch_video_clips.shape)\n",
        "\n",
        "        val_decoder_outputs = model(batch_video_clips, val_translation)\n",
        "        val_translation = val_translation.to(\"cpu\")\n",
        "        val_loss = custom_loss_function(val_translation, val_decoder_outputs)\n",
        "        del val_decoder_outputs\n",
        "\n",
        "        total_val_loss += val_loss.item()\n",
        "\n",
        "# Calculate average validation loss\n",
        "average_val_loss = total_val_loss / len(dev_dataloader)\n",
        "average_train_loss = total_train_loss / len(train_dataloader)\n",
        "train_losses.append(average_train_loss)\n",
        "dev_losses.append(average_val_loss)\n",
        "print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {average_val_loss:.4f}')\n",
        "print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {average_train_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_n6nxoru30m"
      },
      "outputs": [],
      "source": [
        "with open(\"loss_train\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(train_losses, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Ugmhw8u30m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gfslt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
